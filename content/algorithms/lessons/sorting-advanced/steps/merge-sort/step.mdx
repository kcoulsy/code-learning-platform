---
id: merge-sort
title: Merge Sort
order: 1
---

# Merge Sort

Merge sort uses **divide and conquer**: split the array, sort each half, then merge the sorted halves.

## WHY Merge Sort is Important

### The Problem with Simple Sorts

Bubble sort, insertion sort, selection sort: **O(n²)**

For n = 1,000,000:

- O(n²) = 1,000,000,000,000 operations
- Would take hours or days!

Merge sort: **O(n log n)**

For n = 1,000,000:

- O(n log n) = 20,000,000 operations
- Takes seconds

### Key Properties

| Property         | Value                 |
| ---------------- | --------------------- |
| Time Complexity  | O(n log n) always     |
| Space Complexity | O(n)                  |
| Stable           | Yes (preserves order) |
| Divide & Conquer | Yes                   |
| Parallelizable   | Yes                   |

## HOW Merge Sort Works

### The Divide and Conquer Pattern

```
DIVIDE: Split array in half
CONQUER: Recursively sort each half
COMBINE: Merge two sorted halves
```

### Visual Example

```
Split [38, 27, 43, 3, 9, 82, 10]

[38, 27, 43, 3] | [9, 82, 10]

[38, 27] | [43, 3]   [9, 82] | [10]

[38] | [27] | [43] | [3]   [9] | [82] | [10]

MERGE:

[27, 38]     [3, 43]     [9, 10]     [82]
   |            |            |           |
   +-----+------+            +----+------+
         |                       |
      [3, 27, 38, 43]      [9, 10, 82]
                |                   |
                +-----+   +---------+
                      |   |
              [3, 9, 10, 27, 38, 43, 82]
```

### The Merge Process

```
Merge [3, 27, 38, 43] and [9, 10, 82]

         i              j
L: [3, 27, 38, 43]
R: [9, 10, 82]

Compare L[i] and R[j]:
3 < 9 → take 3
   i→

Compare L[i] and R[j]:
27 > 9 → take 9
   j→

Compare L[i] and R[j]:
27 > 10 → take 10
      j→

Compare L[i] and R[j]:
27 < 82 → take 27
   i→

Take remaining: 38, 43, 82

Result: [3, 9, 10, 27, 38, 43, 82]
```

## Pseudo Code: Merge Sort

### Top-Level Algorithm

```
FUNCTION merge_sort(arr, left, right):
    IF left >= right:
        RETURN  // Base case: 0 or 1 element

    mid = left + (right - left) / 2

    merge_sort(arr, left, mid)      // Sort left half
    merge_sort(arr, mid + 1, right) // Sort right half
    merge(arr, left, mid, right)    // Merge sorted halves
```

**WHY this works:**

- Base case: 0 or 1 element is already sorted
- Recursive case: Split and sort halves, then merge
- Each level merges all elements (O(n))
- There are log₂(n) levels
- Total: O(n log n)

### Merge Procedure

```
FUNCTION merge(arr, left, mid, right):
    n1 = mid - left + 1
    n2 = right - mid

    L = array of size n1  // Left half
    R = array of size n2  // Right half

    // Copy to temporary arrays
    FOR i FROM 0 TO n1 - 1:
        L[i] = arr[left + i]

    FOR j FROM 0 TO n2 - 1:
        R[j] = arr[mid + 1 + j]

    // Merge back to arr
    i = 0, j = 0, k = left

    WHILE i < n1 AND j < n2:
        IF L[i] <= R[j]:
            arr[k] = L[i]
            i = i + 1
        ELSE:
            arr[k] = R[j]
            j = j + 1
        k = k + 1

    // Copy remaining elements
    WHILE i < n1:
        arr[k] = L[i]
        i = i + 1
        k = k + 1

    WHILE j < n2:
        arr[k] = R[j]
        j = j + 1
        k = k + 1
```

**WHY merge works:**

- Both L and R are sorted
- Compare front elements, take smaller
- Continue until one array is exhausted
- Copy remaining elements (already sorted)

## Time Complexity Analysis

### Recurrence Relation

```
T(n) = 2T(n/2) + O(n)
     = 2T(n/2) + cn     (for some constant c)
```

### Master Method

For T(n) = aT(n/b) + f(n):

- a = 2 (two subproblems)
- b = 2 (each half size)
- f(n) = O(n) (merge work)

Compare n^log_b(a) with f(n):

- n^log₂(2) = n¹ = n
- f(n) = n = Θ(n^log_b(a))
- Case 2: T(n) = Θ(n log n)

### Visual Cost Breakdown

```
Level 0: 1 merge of n elements = cn
Level 1: 2 merges of n/2 = 2 × c(n/2) = cn
Level 2: 4 merges of n/4 = 4 × c(n/4) = cn
...
Level log n: n merges of 1 element = cn

Total: cn × log n = O(n log n)
```

## Bottom-Up Merge Sort

No recursion, iterative approach:

```
FUNCTION bottom_up_merge_sort(arr):
    n = length(arr)

    // Start with subarrays of size 1
    FOR curr_size FROM 1 TO n - 1:
        // Find start of each pair
        FOR left FROM 0 TO n - 1 BY 2 * curr_size:
            mid = left + curr_size - 1
            right = min(left + 2 * curr_size - 1, n - 1)

            IF mid < right:
                merge(arr, left, mid, right)
```

**Trace for [38, 27, 43, 3, 9, 82, 10]:**

```
Size 1: [38], [27], [43], [3], [9], [82], [10]
Size 2: [27, 38], [3, 43], [9, 82], [10]
Size 4: [3, 27, 38, 43], [9, 10, 82]
Size 7: [3, 9, 10, 27, 38, 43, 82]
```

## Pros and Cons

### Pros

| Advantage                   | Explanation                             |
| --------------------------- | --------------------------------------- |
| **Guaranteed O(n log n)**   | Always performs the same, no worst case |
| **Stable sort**             | Equal elements maintain relative order  |
| **External sorting**        | Works with data too large for memory    |
| **Parallelizable**          | Subarrays can be sorted in parallel     |
| **Linked list friendly**    | No random access needed                 |
| **Predictable performance** | No worst case like quicksort            |

### Cons

| Disadvantage              | Explanation                          |
| ------------------------- | ------------------------------------ |
| **Extra space O(n)**      | Requires temporary arrays            |
| **Not in-place**          | Can't sort with O(1) extra space     |
| **Cache unfriendly**      | Accesses scattered memory            |
| **Recursion overhead**    | Stack frames for each level          |
| **Slower than quicksort** | More comparisons, more memory access |

## When to Use Merge Sort

### Use Merge Sort When:

✓ You need guaranteed O(n log n) performance  
✓ Stability is important (preserve order)  
✓ Sorting linked lists  
✓ Sorting data too large for memory (external sort)  
✓ Parallel processing is available  
✓ Data is nearly sorted (still O(n log n))

### Don't Use Merge Sort When:

✗ Memory is very constrained (use heap sort)  
✗ You need in-place sorting  
✗ Average case speed is priority (use quicksort)  
✗ Array is small (use insertion sort)

## Comparison with Quick Sort

| Aspect          | Merge Sort | Quick Sort      |
| --------------- | ---------- | --------------- |
| Time (worst)    | O(n log n) | O(n²)           |
| Time (average)  | O(n log n) | O(n log n)      |
| Space           | O(n)       | O(log n)        |
| In-place        | No         | Yes             |
| Stable          | Yes        | No              |
| Cache friendly  | No         | Yes             |
| Parallelizable  | Yes        | Yes (with care) |
| Practical speed | Slower     | Faster (cache)  |

## External Merge Sort

For sorting files larger than memory:

```
PHASE 1: Create sorted chunks
--------------------------------
Input file → Read chunk → Sort (merge sort) → Write to temp

PHASE 2: Merge chunks
--------------------------------
Read first element from each chunk
Find smallest → Write to output
Advance that chunk's read position
Repeat until all chunks exhausted
```

**For massive datasets:**

- Use multi-way merge (merge k chunks at once)
- Use disk-efficient merge patterns
- Optimize I/O with buffering

## Key Takeaways

1. **Divide and conquer** - Split, sort, merge
2. **O(n log n) guaranteed** - Unlike quicksort's O(n²) worst case
3. **Merge is the key** - O(n) merge of two sorted halves
4. **Requires O(n) extra space** - Main disadvantage
5. **Stable sort** - Preserves relative order
6. **External sort ready** - Perfect for massive datasets
7. **Recursive or iterative** - Bottom-up avoids recursion overhead
